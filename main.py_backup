# import spacy library
import spacy
import stanfordnlp
# load spacy's english model
nlp = spacy.load('en_core_web_sm')
# stanfordnlp.download('en')
stan_nlp = stanfordnlp.Pipeline()
stan_stop_words = [line.rstrip('\n') for line in open("stopwords.txt")]

# where conditional checks
conditional_array = [ "greater than", "greater", 
    "less than", "lesser", 
    "is equals to", "equals to", "equals", "equal",
    "starts with" ]
conditional_replacement_array = [ ">", ">", 
    "<", "<", 
    "equals", "equals", "equals", "equals",
    "~~" ]

entities = ["student"]
columns = ["id", "name", "age", "mark", "salary", "class"]
exceptions = ["between", "more"]
lemma_exceptions = ["greater", "more"]


# test sentence
# sentence = u'Show all students whose marks greater than 30'
sentence = u'show all students in 10th class whose marks are greater than 30'
# sentence = u'Show all employees with age 30'
# sentence = u'City with maximum number of employees with experience more than 10 years'
# run nlp on sentence
doc = nlp(sentence)

lemmatizedSentence = ''
for token in doc:
    print(token.text, " | ", token.lemma_, " | ", token.pos_, " | ", token.tag_, " | ", token.dep_, " | ",
       token.shape_, " | ", token.is_alpha, " | ", token.is_stop)
    lemmatizedSentence = lemmatizedSentence + (token.text if token.text in lemma_exceptions else token.lemma_) + " "
    # lemmatizedSentence = lemmatizedSentence + token.lemma_ + " "

lemmatizedSentence = lemmatizedSentence.lstrip()

print(lemmatizedSentence)
# stop word removal
spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS

docLemmatized = nlp(lemmatizedSentence)
# removing stop words
tokensLemmatized = [token.text 
            for token in docLemmatized 
            if (
                    not token.is_stop 
                    # or token.is_stop 
                    or token.text in entities 
                    or token.text in columns
                    or token.text in exceptions)]

# tokensLemmatized = [token.text 
#             for token in docLemmatized 
#             ]

print(tokensLemmatized)

# initialization of string to "" 
sanitized_sentence = "" 
# traverse in the string  
for x in tokensLemmatized: 
    sanitized_sentence += x + " "  
sanitized_sentence = sanitized_sentence.lstrip()

doc1 = nlp(sanitized_sentence)

stan_doc = stan_nlp(sanitized_sentence)
stan_doc.sentences[0].print_dependencies()

stan_doc1 = stan_nlp("students in 10th class whose marks greater than 30")

# stan_stop_words
# initialization of string to "" 
stan_tokens_lemma = "" 

stan_tokens_lemma = [token.lemma 
            for token in stan_doc1.sentences[0].words 
            if (
                    not token.lemma in spacy.lang.en.stop_words.STOP_WORDS # stan_stop_words
                    # or token.is_stop 
                    or token.lemma in entities 
                    or token.lemma in columns
                    or token.lemma in exceptions)]

# traverse in the string  
stan_tokens = ""
for x in stan_tokens_lemma: 
    stan_tokens += x + " "  
stan_tokens = stan_tokens.lstrip()
stan_doc2 = stan_nlp(stan_tokens)

print("-----------------------")

# print(*[f'word: {word.text+" "}\tlemma: {word.lemma}' for sent in stan_doc.sentences for word in sent.words], sep='\n')
# print("-----------------------")
# print(*[f'word: {word.text+" "}\tlemma: {word.lemma}' for sent in stan_doc1.sentences for word in sent.words], sep='\n')
# print("-----------------------")
# print(*[f'word: {word.text+" "}\tlemma: {word.lemma}' for sent in stan_doc2.sentences for word in sent.words], sep='\n')

stan_doc2.sentences[0].print_dependencies()

print("-----------------------")
print(sanitized_sentence)
print("-----------------------")

# for entity in doc1.ents:
#     print(entity.text, " | ", entity.start_char, " | ", entity.end_char, " | ", entity.label_)

spacy.displacy.serve(doc1, style="dep")